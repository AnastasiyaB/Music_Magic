{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Music Magic - CS109 Final Project"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Overview:\n",
      "\n",
      "Music production is expensive and often producers would like to know if a song will do well before they agree to record it. Is there a way to establish such a metric? We will look at popular music from the past 5 years, analyzing songs both in terms of their musical quality and lyrical quality as well as factors like popularity, artists involved in the recording and how they are related, how much the song has been shared, and how many people it has reached, etc.\n",
      "\n",
      "\n",
      "### Motivation:\n",
      "We hope to be able to determine what musical qualities lead to a higher artist rating.\n",
      "\n",
      "\n",
      "We were looking to learn more about:\n",
      "\n",
      "* Music composition \n",
      "* How people\u2019s musical preferences may have changed over time\n",
      "* Properly selecting variables that are relevant\n",
      "* Different types of multivariable models (especially regression models) \n",
      "* Different types of classifiers\n",
      "* Making a judgement regarding what explains the data better\n",
      "* Visualizations, especially those we have not used in homeworks\n",
      "* The big question of what makes a song popular\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Related Work\n",
      "\n",
      "Anything that inspired you, such as a paper, a web site, or something we discussed in class.\n",
      "\n",
      "* [Link](http://www.hooktheory.com/blog/i-analyzed-the-chords-of-1300-popular-songs-for-patterns-this-is-what-i-found/). Another introspection into which musical qualities make for a popular song (not quite that since he doesn't compare it to the musical qualities for non-popular songs, neighter is the way he handles data very scientific, but he does have a lot of specific knowledge about music so this was very interesting for us since we thought to combine parts of what he did with non-musical aspects of music to try to make a prediction for artists.\n",
      "\n",
      "* [Link](http://www.gapminder.org/world/#$majorMode=chart$is;shi=t;ly=2003;lb=f;il=t;fs=11;al=30;stl=t;st=t;nsl=t;se=t$wst;tts=C$ts;sp=5.59290322580644;ti=2012$zpv;v=0$inc_x;mmid=XCOORDS;iid=phAwcNAVuyj1jiMAkmq1iMg;by=ind$inc_y;mmid=YCOORDS;iid=phAwcNAVuyj2tPLxKvvnNPA;by=ind$inc_s;uniValue=8.21;iid=phAwcNAVuyj0XOoBL_n5tAQ;by=ind$inc_c;uniValue=255;gid=CATID0;by=grp$map_x;scale=log;dataMin=283;dataMax=110808$map_y;scale=lin;dataMin=18;dataMax=87$map_s;sma=49;smi=2.65$cd;bd=0$inds=;modified=75).Visualization of change over time, that in our opinion was very successful in demonstrating many pieces of information without cluttering. We first saw it shown in a tech talk and thought it would be perfect for the data we are working with: [Here]"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Initial Questions\n",
      "\n",
      "What questions are you trying to answer? How did these questions evolve over the course of the project? What new questions did you consider in the course of your analysis?\n",
      "\n",
      "Initially our questions were skewed heavily towards songs, with most of our questions leading up to one big one: what makes for a successful song? Another example of a question we were excited to answer is: Given Taylor Swift's new album, which song will be the most successful? However, the vast abundancy of data that we looking upon as a great advantange also turned out to be  disadvantage. It is surely impossiblet to use all of it and a lot of the datasets differered vastly in the subset of music that they contained information for. A large part of the process become trying to match up one dataset with another and end up with sometime non-sparse. For our initial analysis of the data, for example, in a frame of 10,000 rows, dropping all rows with either a 0 or a NA value led to a completely empty dataframe!\n",
      "\n",
      "Additionally, a large part of our analysis relied on having ratings for each song. Even after getting permissioned to obtain Yahoo's R2 dataset that contained the most complete information, we struggled to come up with how to match yahoo's song ids to the ids we have already obtained (since no mapping was provided) and dynamically parse through a 5GB dataset, matching it almost to a 2GB dataset. To overcome the issue of sparcity and inability to obtain the proper data on a per-song basis, we altered our original question to focus on the artist -- what kind of general music characteristics make for a higher rated artist?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Data scraping method, cleanup, etc.\n",
      "\n",
      "The majority of the data we used for this project was obtained from Amamzon's Million Song Dataset, Yahoo Labs, and scraped from (ALEX INSERT LINK HERE). Let's get started with obtaining the data:\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Download the \"Million Song Subset\" from...\n",
      "http://static.echonest.com/millionsongsubset_full.tar.gz\n",
      "\n",
      "\n",
      "Extract and place the \"MillionSongSubset\" directory inside your git repo directory (which should be named \"Music_Magic\")\n",
      "\n",
      "Add your basepath below (top of the next cell).\n",
      "\n",
      "My full path to the subset is '/Users/csmiles/Dropbox/Classes/Statistics 121/Music_magic/MillionSongSubset'\n",
      "So my basepath is '/Users/csmiles/Dropbox/Classes/Statistics 121'\n",
      "\n",
      "Comment out the basepaths that aren't yours.\n",
      "\n",
      "Run the code below and start playing around! The code is taken from a tutorial on the Million Song Dataset website, \n",
      "and should give you a good sense of how to use the Python wrapper they provide (it's in the directory \n",
      "                                                                                '/Music_Magic/MSongsDB/PythonSrc', which I pushed to the repository so you don't need to download it manually). There's examples of how to iterate over all the songs, how to use the \"Additional Files\" (which include SQL databases for quick lookups), etc. Still read over the information about the Million Song Subset on the website if you have time though.\n",
      "\n",
      "#### One last thing: \n",
      "\n",
      "I also tried to push a .gitignore file which should've been added to your 'Music_Magic' repo when you pulled. It's really important to have this, otherwise we might accidentally push the entire Million Song Subset (which is still pretty huge) to the repo. If for some reason you don't get the .gitignore file when you pull, maybe try to download it manually from GitHub (it's also only one line, so you could write it yourself) before you commit and push any changes. Remember that it's a hidden file so it won't appear in the regular file browser!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Instructions on how to set up:\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# SET YOUR BASEPATH\n",
      "#basepath = '/Users/csmiles/Dropbox/Classes/Statistics 121' # Chris\n",
      "basepath = '/Users/anastasiyaborys/Desktop/cs109/Final_Project' # Anastasiya\n",
      "\n",
      "# imports\n",
      "import os\n",
      "import sys\n",
      "import time\n",
      "import glob\n",
      "import datetime\n",
      "import sqlite3\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "from pylab import *\n",
      "import scipy as sp\n",
      "from scipy import stats\n",
      "from matplotlib import rcParams\n",
      "\n",
      "%matplotlib inline\n",
      "\n",
      "ext='.h5'\n",
      "pd.set_option('display.width', 500)\n",
      "pd.set_option('display.max_columns', 30)\n",
      "\n",
      "# path to the Million Song Dataset subset\n",
      "msd_subset_path = basepath + '/Music_Magic/MillionSongSubset'\n",
      "msd_subset_data_path = os.path.join(msd_subset_path, 'data')\n",
      "msd_subset_addf_path = os.path.join(msd_subset_path, 'AdditionalFiles')\n",
      "assert os.path.isdir(msd_subset_path), 'wrong path'\n",
      "\n",
      "# path to the Million Song Dataset code\n",
      "msd_code_path = basepath + '/Music_Magic/MSongsDB'\n",
      "assert os.path.isdir(msd_code_path), 'wrong path' \n",
      "\n",
      "# we add some paths to python so we can import MSD code\n",
      "sys.path.append(os.path.join(msd_code_path, 'PythonSrc'))\n",
      "\n",
      "# imports specific to the MSD\n",
      "import hdf5_getters as GETTERS\n",
      "\n",
      "# the following function simply gives us a nice string for\n",
      "# a time lag in seconds\n",
      "def strtimedelta(starttime,stoptime):\n",
      "    return str(datetime.timedelta(seconds=stoptime-starttime))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AssertionError",
       "evalue": "wrong path",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-6-8af47dbf5da1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# path to the Million Song Dataset code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mmsd_code_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbasepath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/Music_Magic/MSongsDB'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsd_code_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wrong path'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# we add some paths to python so we can import MSD code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAssertionError\u001b[0m: wrong path"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# remove border helper we have been using all along\n",
      "\n",
      "#these colors come from colorbrewer2.org. Each is an RGB triplet\n",
      "dark2_colors = [(0.10588235294117647, 0.6196078431372549, 0.4666666666666667),\n",
      "                (0.8509803921568627, 0.37254901960784315, 0.00784313725490196),\n",
      "                (0.4588235294117647, 0.4392156862745098, 0.7019607843137254),\n",
      "                (0.9058823529411765, 0.1607843137254902, 0.5411764705882353),\n",
      "                (0.4, 0.6509803921568628, 0.11764705882352941),\n",
      "                (0.9019607843137255, 0.6705882352941176, 0.00784313725490196),\n",
      "                (0.6509803921568628, 0.4627450980392157, 0.11372549019607843),\n",
      "                (0.4, 0.4, 0.4)]\n",
      "\n",
      "rcParams['figure.figsize'] = (10, 6)\n",
      "rcParams['figure.dpi'] = 150\n",
      "rcParams['axes.color_cycle'] = dark2_colors\n",
      "rcParams['lines.linewidth'] = 2\n",
      "rcParams['axes.grid'] = False\n",
      "rcParams['axes.facecolor'] = 'white'\n",
      "rcParams['font.size'] = 14\n",
      "rcParams['patch.edgecolor'] = 'none'\n",
      "\n",
      "\n",
      "def remove_border(axes=None, top=False, right=False, left=True, bottom=True):\n",
      "    \"\"\"\n",
      "    Minimize chartjunk by stripping out unnecessary plot borders and axis ticks\n",
      "    \n",
      "    The top/right/left/bottom keywords toggle whether the corresponding plot border is drawn\n",
      "    \"\"\"\n",
      "    ax = axes or plt.gca()\n",
      "    ax.spines['top'].set_visible(top)\n",
      "    ax.spines['right'].set_visible(right)\n",
      "    ax.spines['left'].set_visible(left)\n",
      "    ax.spines['bottom'].set_visible(bottom)\n",
      "    \n",
      "    #turn off all ticks\n",
      "    ax.yaxis.set_ticks_position('none')\n",
      "    ax.xaxis.set_ticks_position('none')\n",
      "    \n",
      "    #now re-enable visibles\n",
      "    if top:\n",
      "        ax.xaxis.tick_top()\n",
      "    if bottom:\n",
      "        ax.xaxis.tick_bottom()\n",
      "    if left:\n",
      "        ax.yaxis.tick_left()\n",
      "    if right:\n",
      "        ax.yaxis.tick_right()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have a subset of 10,000 songs so let's build a dataframe with the information we want:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_df = pd.DataFrame(columns=('Song_ID', 'Artist_ID', 'Title','Song_Hotness','Danceability', 'Duration', 'Loudness', 'Year', \n",
      "                                'Analysis_Sample_Rate','Song_Energy', 'Album_Name', 'Bars_Confidence', 'Key', 'Mode',\n",
      "                                'Mode_confidence', 'Tempo', 'Artist_Name', 'Artist_location', 'Artist_Hotness', 'Similar_Artists',\n",
      "                                'Artist_Tags'))\n",
      "print 'Dataframe created with the following rows:' , data_df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "basedir = msd_subset_data_path\n",
      "for root, dirs, files in os.walk(basedir):\n",
      "    files = glob.glob(os.path.join(root,'*'+ext))\n",
      "    for f in files :\n",
      "            \n",
      "        h5 = GETTERS.open_h5_file_read(f) \n",
      "    \n",
      "        # song information\n",
      "        song_id = GETTERS.get_song_id(h5) \n",
      "        song_title = GETTERS.get_title(h5)\n",
      "        song_hotness = GETTERS.get_song_hotttnesss(h5)\n",
      "        song_dancability = GETTERS.get_danceability(h5)\n",
      "        song_duration = GETTERS.get_duration(h5)\n",
      "        song_loudness = GETTERS.get_loudness(h5)\n",
      "        song_year = GETTERS.get_year(h5)\n",
      "        analysis_sample_rate = GETTERS.get_analysis_sample_rate(h5)\n",
      "        energy = GETTERS.get_energy(h5)\n",
      "        bars_confidence = GETTERS.get_bars_confidence(h5)\n",
      "        key = GETTERS.get_key(h5)\n",
      "        mode = GETTERS.get_mode(h5)\n",
      "        mode_confidence = GETTERS.get_mode_confidence(h5)\n",
      "        tempo = GETTERS.get_tempo(h5)\n",
      "           \n",
      "        # artist information\n",
      "        album_name = GETTERS.get_release(h5)\n",
      "        artist_id = GETTERS.get_artist_id(h5)\n",
      "        artist_name = GETTERS.get_artist_name(h5)\n",
      "        artist_location = GETTERS.get_artist_location(h5)\n",
      "        artist_hotness = GETTERS.get_artist_hotttnesss(h5)\n",
      "        similar_artists = GETTERS.get_similar_artists(h5)\n",
      "        artist_tags = GETTERS.get_artist_mbtags(h5)\n",
      "    \n",
      "            \n",
      "        data_df = data_df.append([dict(Song_ID = song_id, Artist_ID = artist_id, Title=song_title, \n",
      "                                       Song_Hotness=song_hotness, Danceability=song_dancability, \n",
      "                                       Duration=song_duration, Loudness=song_loudness, Year=song_year,\n",
      "                                       Album_Name = album_name, Bars_Confidence = bars_confidence,\n",
      "                                       Mode = mode, Key = key,\n",
      "                                       Mode_confidence = mode_confidence, Tempo = tempo,\n",
      "                                       Analysis_Sample_Rate = analysis_sample_rate, Song_Energy = energy,\n",
      "                                       Artist_Name=artist_name, Artist_location=artist_location, \n",
      "                                       Artist_Hotness=artist_hotness, Similar_Artists=similar_artists,\n",
      "                                       Artist_Tags=artist_tags)], ignore_index=True)\n",
      "        h5.close()      \n",
      "   \n",
      "print \"Done\"\n",
      "print data_df\n",
      "# so that we don't have to keep rerunning the code\n",
      "data_df.to_csv('all_data.csv', index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# generate the data we need from the yahoo dataset\n",
      "# Chris please put in more description about what you're doing here\n",
      "name_aid_dict = {}\n",
      "\n",
      "f = open(basepath + '/Music_Magic/YahooR1/output.txt','r')\n",
      "for line in f.xreadlines():\n",
      "    parts = line.strip().split('<SEP>')\n",
      "    name_aid_dict[parts[0]] = parts[1]\n",
      "f.close()\n",
      "\n",
      "yid_name_dict = {}\n",
      "\n",
      "f = open(basepath + '/Music_Magic/YahooR1/ydata-ymusic-artist-names-v1_0.txt','r')\n",
      "for line in f.xreadlines():\n",
      "    parts = line.strip().split('\\t')\n",
      "    yid_name_dict[parts[0]] = parts[1]\n",
      "f.close()\n",
      "\n",
      "aids, names, yids, ratings, uids = [],[],[],[],[]\n",
      "\n",
      "# read yahoo data\n",
      "f = open(basepath + '/Music_Magic/YahooR1/ydata-ymusic-user-artist-ratings-v1_0.txt','r')\n",
      "for line in f.xreadlines():\n",
      "    parts = line.strip().split('\\t')\n",
      "    yid = parts[1]\n",
      "    if yid in yid_name_dict:\n",
      "        name = yid_name_dict[yid]\n",
      "        if name in name_aid_dict:\n",
      "            aid = name_aid_dict[name]\n",
      "            aids.append(aid)\n",
      "            names.append(name)\n",
      "            yids.append(yid)\n",
      "            ratings.append(parts[2])\n",
      "            uids.append(parts[0])\n",
      "f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "yahoo_dataframe = pd.DataFrame({'aids': aids, 'names': names, 'yids': yids, 'ratings': ratings, 'uids': uids})\n",
      "yahoo_dataframe.to_csv('yahoo_ratings.csv', index = False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load both dataframes in\n",
      "data_df = pd.read_csv(\"all_data.csv\")\n",
      "yahoo_dataframe = pd.read_csv(\"~/Volumes/USB\\ DISK/yahoo_ratings.csv\")\n",
      "# import the rat\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IOError",
       "evalue": "File all_data.csv does not exist",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-7-51c69eccbe3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load both dataframes in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"all_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0myahoo_dataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"~/Volumes/USB\\ DISK/yahoo_ratings.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# import the rat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/anastasiyaborys/anaconda/python.app/Contents/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols)\u001b[0m\n\u001b[1;32m    398\u001b[0m             )\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/anastasiyaborys/anaconda/python.app/Contents/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/anastasiyaborys/anaconda/python.app/Contents/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/anastasiyaborys/anaconda/python.app/Contents/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/anastasiyaborys/anaconda/python.app/Contents/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m    955\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/anastasiyaborys/anaconda/python.app/Contents/lib/python2.7/site-packages/pandas/parser.so\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:2987)\u001b[0;34m()\u001b[0m\n",
        "\u001b[0;32m/Users/anastasiyaborys/anaconda/python.app/Contents/lib/python2.7/site-packages/pandas/parser.so\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:5345)\u001b[0;34m()\u001b[0m\n",
        "\u001b[0;31mIOError\u001b[0m: File all_data.csv does not exist"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Exploratory Analysis:\n",
      "What visualizations did you use to look at your data in different ways? What are the different statistical methods you considered? Justify the decisions you made, and show any major changes to your ideas. How did you reach these conclusions?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "*css tweaks in this cell*\n",
      "<style>\n",
      "div.text_cell_render {\n",
      "    line-height: 150%;\n",
      "    font-size: 110%;\n",
      "    width: 800px;\n",
      "    margin-left:50px;\n",
      "    margin-right:auto;\n",
      "    }\n",
      "</style>\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}